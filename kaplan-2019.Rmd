---
title: "A unified introduction to inference"
author: "Daniel T. Kaplan"
date: "June 25, 2019"
output: 
  html_document: 
    fig_caption: yes
    number_sections: yes
---

```{r include = FALSE}
library(mosaic)
library(tidyverse)
library(mosaicModel)
our_theme <- theme_bw()
```

A conventional approach to teaching statistical inference in a first university-level course refers to a series of statistics and their standard errors: the difference between two proportions, the difference between two means, and the slope of a regression line. Prior to this, the concept of a standard error is introduced in the context of a sample proportion and sample mean. 

The challenges faced by students in this conventional approach are well known to instructors: the formulas for the standard errors are complicated; the formulas are different in the various settings, yet similar enough to be mistaken for one another; the connection of the formulas to the underlying idea of sampling variation is not obvious; the concept of a sampling distribution is nuanced; and the repeated use of the words sample, standard, error, statistic in the vocabulary -- standard deviation, standard error, margin of error, sample statistic, sampling variation, test statistic -- adds cognitive load.

Subsequent topics in inference are handled differently. In the chi-squared test and ANOVA, there is no standard error nor is there a confidence interval resulting from the process, just a p-value. This creates additional potential for confusion and draws undue attention to the p-value, which is the one quantity that appears in all of the inferential settings.

A recent trend is to unify the procedures of inference across the various settings by basing them on a couple of conceptually simple operations: resampling and shuffling. (See Lock5, Tintle.) This simulation-based approach is intrinsically rooted in computing that cannot practicably be done on a calculator or in a spreadsheet. Instead, the computing can be done using professional-level software packages (such as R) or custom-built, interactive web apps. (See software sites for Lock 5, Tintle.) 

This note describes another way of unifying inference procedures by adopting a standard graphical presentation and calculations of confidence intervals and p-values without explicit reference to the standard error. Section 2 describes the graphical presentation and how it accomodates each of the inference settings in the conventional approach. Section 3 shows how confidence intervals and p-values can be calculated in a straightforward way without computing a standard error.
Section 4 describes a web-based, interactive app to support exploration and use of the new approach and ways in which the new approach can help address broad problems with conventional inference described by the GAISE and ASA p-value reports. [CITE]  [ALSO IN SECTION 4, putting the statistics always in the context of the data.] Section 5 anticipates and addresses possible criticisms of the new approach. (?? Appendix: ggformula commands??)

# A unifying graphical presentation

To start, it helps to make a small change in nomenclature. In the conventional approach to inference, the phrases "response variable" and "explanatory variable" or their equivalent are used with simple regression, while the difference-between-means and difference-between-proportions settings refer to "two samples," even though there is really one sample with two variables: a response variable and a dicotomous explanatory variable that defines the two groups whose means or proportions are being compared. We use "response" and "explanatory" for all settings. (Later, we will use "covariate" to stand for a second explanatory variable.)

The unifying graphic is a point plot of the response variable versus the response variable. The various conventional inference settings differ in whether the response and explanatory variables are numerical or dicotomous categorical variables:

a. Difference between means: response is numeric, explanatory is dicotomous categorical.
b. Difference between proportions: response is dicotomous categorical as is the explanatory variable.
c. Simple regression: response is numeric as is the explanatory variable.

Two complete the set of combinations of variable types, we add in another setting which is common in statistical applications but missing from the canonical set found in introductory textbooks:

d. Binary regression: response is dicotomous categorical, response is numeric.

In contemporary statistical graphics, (e.g. [cite grammar of graphics, ggplot2 book]) a categorical variable in a point plot is represented by "dodging": each level is assigned a discrete position on the coordinate axis. Other useful techniques in forming the point plot are "jittering" and transparency. Jittering displaces each point by a small random perturbation from it's assigned discrete position. Jittering and transparency can be used together to avoid overplotting one data point on another, thereby making it clear to the eye the density of points at each location.

Using dodging, jittering, and transparency as needed, the four inference settings above can be effectively shown in a point plot, as shown in Figures 1 a-d, which displays some numerical and categorical variables from the `mosaicData::CPS85` data frame. [CITE mosaic Data and give link to direct URL of CSV]

```{r preliminaries, echo = FALSE}
CPS85 <- CPS85 %>% filter(wage < 30) %>%
  mutate(union_num = as.numeric(union) +  runif(nrow(.), -0.2, 0.2))
CPS85$mod1 <- mod_eval(lm(educ ~ sex, data = CPS85), data = CPS85)$model_output
CPS85$mod1r <- CPS85$mod1 + runif(nrow(CPS85), -0.0002, 0.0002)
CPS85$mod2 <- mod_eval(lm(as.numeric(union) ~ sex, data = CPS85), data = CPS85)$model_output
CPS85$mod2r <- CPS85$mod2 + runif(nrow(CPS85), -0.005, 0.005)
CPS85$mod3 <- mod_eval(lm(educ ~ age, data = CPS85), data = CPS85)$model_output
CPS85$mod3r <- CPS85$mod3 + runif(nrow(CPS85), -0.1, 0.1)
CPS85$mod4 <- mod_eval(lm(as.numeric(union) ~ age, data = CPS85), data = CPS85)$model_output
#CPS85$mod4r <- CPS85$mod4 + runif(nrow(CPS85), -0.01, 0.01)
```

```{r data-plot, echo = FALSE, fig.show = "hold", out.width = "50%", fig.cap = "Figure 1. Point plots in various settings for inference."}

Stats_wage <- df_stats( ~ wage, data = CPS85, mean = mean, sd = sd) %>%
  mutate(lower = mean - 2 * sd, upper = mean + 2 * sd)
Stats_union <- df_stats( ~ as.numeric(union), data = CPS85,
                         mean = mean, sd = sd) %>%
  mutate(lower = mean - 2 * sd, upper = mean + 2 * sd)
P1 <- 
  gf_jitter(educ ~ sex, data = CPS85, 
          width = 0.2, seed = 101, alpha = 0.5) %>%
  gf_theme(our_theme) %>%
  gf_labs(title = "(a) quantitative vs categorical") %>%
  gf_segment(educ + educ ~ 2.5 + 2.55, shape = "-", alpha = 0.5, width = 2) %>%
  gf_pointrange(mean + lower + upper ~ 2.475, data = Stats_wage, color = "red", size = 1.5, alpha = 0.5)

P2 <- 
  gf_jitter(union ~ sex, data = CPS85, 
          width = 0.2, height = 0.2, seed = 101, alpha = 0.5) %>%
  gf_theme(our_theme) %>%
  gf_labs(title = "(b) categorical vs categorical") %>%
  gf_segment(union_num + union_num ~ 2.5 + 2.55, shape = "-", alpha = 0.5, width = 2) %>%
  gf_pointrange(mean + lower + upper ~ 2.475, data = Stats_union, color = "red", size = 1.5, alpha = 0.5)

P3 <- gf_jitter(educ ~ age, data = CPS85, 
         alpha = 0.5) %>%
  gf_theme(our_theme) %>%
  gf_labs(title = "(c) quantitative vs quantitative") %>%
  gf_segment(educ + educ ~ 71 + 72, shape = "-", alpha = 0.5, width = 2) %>%
  gf_pointrange(mean + lower + upper ~ 70.5, data = Stats_wage, color = "red", size = 1.5, alpha = 0.5)

P4 <- 
  gf_jitter(union ~ age, data = CPS85, 
         alpha = 0.5, height = 0.2) %>%
  gf_theme(our_theme) %>%
  gf_labs(title = "(d) categorical vs quantitative") %>%
  gf_segment(union_num + union_num ~ 71 + 72, shape = "-", alpha = 0.5, width = 2) %>%
  gf_pointrange(mean + lower + upper ~ 70.5, data = Stats_union, color = "red", size = 1.5, alpha = 0.5)

P1; P2; P3; P4
```

In addition to the point plot itself, each graphic includes a "rug" plot of the values of the response variable along with a "point-range" bar showing the mean $\pm 2$ standard deviation of those values. This summary of the response will be used later in the inference calculations.

Another change in nomenclature helps unifying the sample statistic in the various conventional inference settings. Rather than referring to groupwise means, groupwise proportions, and slopes, we'll display "model values" of the response variable as a function of the explanatory variable. The model value for a particular data point corresponds to the mean response for that point's group or the value of a regression line at that point's explanatory value. The corresponding plot layer (Figure 2) is similar in format to the data plot.

```{r model-plot, echo = FALSE, fig.show = "hold", out.width = "50%", fig.cap = "Figure 2. Model values in the various settings for inference."} 

Stats_mod1 <- 
  df_stats( ~ mod1, data = CPS85, mean = mean, sd = sd) %>%
  mutate(lower = mean - 2 * sd, upper = mean + 2 * sd)
Stats_mod2 <- 
  df_stats( ~ mod2, data = CPS85, mean = mean, sd = sd) %>%
  mutate(lower = mean - 2 * sd, upper = mean + 2 * sd)
Stats_mod3 <- 
  df_stats( ~ mod3, data = CPS85, mean = mean, sd = sd) %>%
  mutate(lower = mean - 2 * sd, upper = mean + 2 * sd)
Stats_mod4 <- 
  df_stats( ~ mod4, data = CPS85, mean = mean, sd = sd) %>%
  mutate(lower = mean - 2 * sd, upper = mean + 2 * sd)

PM1 <- function(x = NULL) {
  x %>%
    gf_jitter(mod1r ~ sex, data = CPS85, 
              width = 0.2, height = 0.0, 
              color = "blue", seed = 101, alpha = 0.5) %>%
    gf_theme(our_theme) %>%
    gf_labs(title = "(a) quantitative vs categorical") %>%
    gf_segment(mod1r + mod1r ~ 2.3 + 2.35, shape = "-", alpha = 0.5, width = 2) %>%
    gf_pointrange(mean + lower + upper ~ 2.375, data = Stats_mod1,
                  color = "blue", size = 1.5, alpha = 0.5)
}

PM2 <- function(x = NULL) {
  x %>%
    gf_jitter(mod2r ~ sex, data = CPS85, 
              width = 0.2, height = 0.0, 
              color = "blue",
              seed = 101, alpha = 0.5) %>%
    gf_theme(our_theme) %>%
    gf_labs(title = "(b) categorical vs categorical") %>%
    gf_segment(mod2r + mod2r ~ 2.3 + 2.35, shape = "-", alpha = 0.5, width = 2) %>%
    gf_pointrange(mean + lower + upper ~ 2.375, data = Stats_mod2,
                  color = "blue", size = 1.5, alpha = 0.5)
}
PM3 <- function(x = NULL) {
  x %>%
    gf_point(mod3 ~ age, data = CPS85, 
             alpha = 0.5, color = "blue") %>%
    gf_theme(our_theme) %>%
    gf_labs(title = "(c) quantitative vs quantitative") %>%
    gf_segment(mod3 + mod3 ~ 67 + 68, shape = "-", 
               alpha = 0.5, width = 2) %>%
    gf_pointrange(mean + lower + upper ~ 68.5, data = Stats_mod3,
                  color = "blue", size = 1.5, alpha = 0.5)
}

PM4 <- function(x = NULL) {
  x %>% 
    gf_point(mod4 ~ age, data = CPS85, 
              color = "blue", alpha = 0.5, height = 0.0) %>%
    gf_theme(our_theme) %>%
    gf_labs(title = "(d) categorical vs quantitative") %>%
    gf_segment(mod4 + mod4 ~ 67 + 68, shape = "-", alpha = 0.5, width = 2) %>%
    gf_pointrange(mean + lower + upper ~ 68.5, data = Stats_mod4,
                  color = "blue", size = 1.5, alpha = 0.5)
} 

PM1() %>% gf_labs(y = "Model value of wage") 
PM2() %>% gf_labs(y = "Model value of union") 
PM3() %>% gf_labs(y = "Model value of wage") 
PM4() %>% gf_labs(y = "Model value of union") 
```

For a dicotomous response value, the model values are calculated using a 0-1 coding. In this coding, means are equivalent to proportions.

A striking feature of Figure 2 is that there are only two "shapes" for the plot, even though there are four settings for the models. The reason is that the vertical axis is being automatically scaled to the range of the model values. Every plot of model values will have one of these two shapes -- two horizontal bars for a categorical explanatory variable and a line for a numeric explanatory variable, independent of the response variable. The only variations are whether the slope of the line is positive or negative (or zero), or whether the left bar is higher or lower (or the same) as the right bar.


```{r both-plot, echo = FALSE, fig.show = "hold", out.width = "50%", fig.cap = "Figure 3. Showing both data and model values."} 
P1 %>% PM1()
P2 %>% PM2()
P3 %>% PM3()
P4 %>% PM4()
```


It is only when the model-value plot is overlaid on the same scale as the data plot that the regression contexts become apparent, as in Figure 3, which is the style of plot I recommend for teaching inference.


# F is for inference

Several descriptive statistics can be visualized from the graphical format in the previous section: groupwise means and proportions and their differences, the slope of a regression line, the standard deviation of the response variable and of the model values. These, together with the sample size $n$ are the basic inputs to the calculations for formal inference.

For notation, I'll use $B$ to stand for the (unstandardized) effect size. This corresponds to the difference in means, the difference in proportions, or the slope of the regression line, depending on the context of the problem. The sample standard deviation of the response variable will be denoted as $s_{raw}$ while the standard deviation of the model values will be $s_{model}$.

A basic inferential statistic applicable to all settings is the coefficient of determination, R^2^, which is equal to the square of the ratio of the model and raw standard deviations:

$$\mbox{R}^2 = s^2_{model} / s^2_{raw} .$$
An important inferential statistic based on R^2^ and sample size $n$ is the F-statistic. For the settings (a) through (d), the F statistic is

$$\mbox{F} = (n-1) \frac{\mbox{R}^2}{1 - \mbox{R}^2} .$$

The 95% confidence interval on the effect size B is simply expressed in terms of F:

$$\mbox{95% confidence interval} = B (1 \pm 2 / \sqrt{F}). $$

The 2 in the formula for the confidence interval corresponds to the tradition 1.96 for 95% confidence from the normal distribution.

In the conventional approach to inference, a variety of test statistics is used: z for the difference in proportions, t for the difference in mean or regression slope, F for ANOVA and multiple regression. Given these different scales, it's sensible to summarize the results of a null hypothesis test with a common scale: the p-value which puts all results on a scale of 0 to 1.

In the proposed approach, F is used for all inference settings. Following the traditional practice, F can be translated to a  p-value can be determined by reference to a table, software, or simply the graph in Figure XXXX. There are $n-1$ degrees of freedom. On the other hand, since F is used for all of the inference tests, it's sensible just to use F as the measure of implausibility of the null hypothesis. $\mbox{F} > 4$ corresponds to  $p < 0.05$ while $\mbox{F} > 7$ corresponds to $p < 0.01$.

The graphical format of situations (a) through (d) is readily generalized to include multiple levels for the categorical explanatory variable enabling "one-way" ANOVA to be handled with the same apparatus. 

There is a small change to the formula for F. For $k$ groups, 
$$\mbox{F} = \frac{n-(k-1)}{k-1} \frac{\mbox{R}^2}{1 - \mbox{R}^2} .$$

(See Section XXXXX pedagogy XXXXX for suggestions on explaining this change to students.)

IMAGES: 

* One-way ANOVA
* Two explanatory variables

The same graphical format can also be used for multiple regression. One way is to represent the second explanatory variable by discrete colors (when the second variable is categorical) or by a color gradient (when the second variable is numerical). F calculated in the manner described above corresponds to the correct value of the whole-model p-value. However, there is no simple formula for confidence intervals on the multiple-regression model coefficients.

# Pedagogy

The primary goals of the proposed approach to statistical inference are:

1. to simplify the topic while keeping it genuine by reducing the several settings of the traditional approach to a single setting that can be handled by a single graphical format.
2. to smooth the path for students to work with two explanatory variables
 

Over the last decade, there has been growing recognition in the statistical education community in incorporating settings with multiple explanatory variables in introductory university-level statistics. [See e.g. Kaplan, Horton, Shield] The 2016 Guidelines for Assessment and Instruction in Statistics Education (GAISE cite). The first GAISE recommendation includes "Give students experience with multivariable thinking."

The graphic at the heart of the approach can be constructed with statistical software (such as R) or by a simple web application that requires no installation of software and no experience with code. (A draft of such a web app is available at <https://dtkaplan.shinyapps.io/LA_explain/>.) 

A distinctive feature of lower-level university mathematics and statistics is that many instructors largely eschew modern computing in favor of a calculator. Among the justifications given for this practice are: 1. the lack of availability of computing infrastructure beyond calculators; 2. the belief  -- right or wrong -- that drill with hand computation of quantities such as the mean and standard deviation informs a student's understanding of these quantities; 3. using calculators helps to avoid students cheating on exams, since calculators generally lack the communications capabilities of computers. 

One consequence of the exclusive use of calculators for teaching inference is that the formulas of the traditional approach are seen as essential components of statistical procedure, since the value of those formulas can be worked out by plugging in numerical estimates of simple, "sufficient" statistics such as the mean and standard deviation.

The proposed approach to inference should be particularly attractive to those who avoid computing in their statistics classes. The standard graphic of the approach (Figure 3) can be generated by a web app or can be presented to students in a printed format.

With a little practice, the value of $R \equiv \sqrt{R^2}$ can be read by eye directly from the graphic by comparing the length of the point-line marker for the model values to that of the raw values of the response variable.

The remaining calculations involved in statistical inference can also be accomplished by eye, using graphs that encapsulates the formulas and probability tables of the traditional approach. Three such graphs may be particularly useful:

1. Calculating the value of F from R and n. (Figure 4)
2. Computing from F and n confidence intervals of the effect size B. (Figure 5)
3. Calculating p-values from F and n. (Figure 6)

```{r F-graph, echo = FALSE, message = FALSE, fig.cap="Figure 4. Computing F from R and n"}
make_vals <- function(n) {
  tibble::tibble(R = seq(0.05, 0.95, length = 100), 
             samp_size = n,
             F = (n - 1) * R^2 / (1 - R^2),
             ci.lower = 1 - 2 / sqrt(F),
             ci.upper = 1 + 2 / sqrt(F),
             p = 1 - pf(F, 1, n)) %>%
    mutate(samp_size = factor(samp_size))
}
Vals <- rbind(
  make_vals(5),
  make_vals(10),
  make_vals(20),
  make_vals(50),
  make_vals(100),
  make_vals(200),
  make_vals(500)
)
gf_line(F ~ R, color = ~ samp_size, 
        data = Vals %>% filter( ! samp_size %in% c("5", "10"))) %>%
  gf_refine(scale_y_log10(
    breaks = c(1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 
               300, 500, 1000, 2000, 3000, 5000, 10000 )),
    scale_x_continuous(breaks = seq(0.1, 0.9, by = 0.1))) %>%
  gf_theme(our_theme) %>%
  gf_labs(y = "F", title = "F from R and n") +
  guides(color = guide_legend(reverse = TRUE))
```



```{r ci-from-F, echo = FALSE, message = FALSE, warning = FALSE, fig.cap = "Figure 5: Confidence intervals from F. Multiply the effect size B by the upper and lower bounds at the appropriate F value. The central blue band is for large n. Bounds are also shown for several small n."}
Fringe_f <- Vals %>% filter(samp_size %in% c(5, 10, 20), F < 20)
multipliers <- data.frame(samp_size = c("5", "10", "20"), 
                          mult = c(6.608, 4.96, 4.35) / 3.85)
Fringe_f <- Fringe_f %>% left_join(multipliers) %>%
  mutate(top = 1 + (ci.upper-1) * mult, 
         bottom = 1 - (top - 1))
gf_ribbon(ci.lower + ci.upper ~ F, fill = "blue", 
          data = Vals %>% filter(samp_size == "100", F < 20), 
          alpha = 0.3) %>%
  gf_line(top ~ F, color = ~ samp_size, data = Fringe_f, inherit = FALSE) %>%
  gf_line(bottom ~ F, color = ~ samp_size, data = Fringe_f, inherit = FALSE) +
  scale_x_continuous(breaks = 1:20) +
  scale_y_continuous(
    breaks = seq(-1, 3, by = 1),
    labels = c("-B", "0", "B", "2B", "3B"))  + 
  ylab("CI bounds") +
  ylim(c(-1, 3)) +
  theme_bw()
```



```{r echo = FALSE, fig.cap="Figure 6. p-values from F and n. 0.05 critical values are shown for n ≤ 10.", warning = FALSE}
Small_n <- data.frame(
  x = qf(.95, 1, 3:9),
  y = 0.05
)
Fframe = list()
for (n in c(5, 10, 20, 50, 100, 200)) {
  Fframe[[n]] <- 
    data.frame(F = seq(2, 12, by = .1)) %>%
    mutate(p = 1 - pf(F,  1, n-1),
           n = n)
}
Fframe <-  dplyr::bind_rows(Fframe) %>%
  mutate(n = factor(n))
gf_line(p ~ F, data = Fframe, color  = ~n) %>%
  gf_point(y ~ x, data = Small_n, inherit = FALSE, shape = 3) +
  scale_y_log10(breaks = c(0.1, 0.05, .002, 0.01, .005,  0.001),
                minor_breaks = NULL) +
  scale_x_continuous(breaks = 2:12) +
  theme_bw()
```

These three graphs and an example from Figure 3 can be placed on the two sides of an ordinary index card and provided to students as the computing infrastructure needed for inference.

# Pros and cons

The traditional approach to introductory inference was developed over many decades. Scores of textbook authors have had the opportunity to contribute their improvements and perspectives. The most recent major innovation is the use of randomization procedures to introduce inference. [Cite Lock, Tintle, Open Intro]. It's a truism that almost everyone teaching statistics who has significant formal training in that field has seen and mastered, to the extent possible, the traditional approach. A very substantial fraction of instructors teaching introductory statistics, including this author, were not trained in statistics and learned the material from the textbook used in class.

It's natural that people who are familiar with the traditional approach think about the underlying problem in those terms. For these people, any substantial deviation from the tried and true will seem initially more difficult. In addition, there are components of the proposed approach that will be unfamiliar to the many instructors whose training consists of teaching an introductory course. For example, R^2^ is not encountered in a many textbooks' chapters up through simple regression. Instead the emphasis is on the Pearson product-moment correlation coefficient, r. Many textbooks have a summation formula process for calculating F; the calculation via R^2^ is not seen in many introductory textbooks. Of dozens of statistics educators to whom I've demonstrated the proposed approach of calculating confidence intervals from F, none had any initial idea that this was possible. (It helps to remind instructors that, with one degree of freedom in the numerator, F = t^2. And to point out that t is the effect size B divided by the standard error. Thus, F is linked to the standard error and the confidence interval.)

Of course, our students initially have no such mastery of the traditional approach. Properly judging the proposed approach's difficulty for students must necessarily involve trying it in the classroom. Encouraging instructors who see face validity to the proposed approach to try it in the classroom is the major purpose of this article.

As a mathematical object of study, there's nothing about the traditional approach that needs fixing. But that object was developed to help researchers deal with contemporary problems, and the nature of contemporary problems has changed substantially in the many decades since the traditional approach was developed. In this regard, it's worth noting that approaches to inference outside of the tradition, e.g. statistical/machine learning are widely taught with an entirely non-traditional infrastructure, for instance, cross-validation. [cite: Machine era statistical inference.]

Putting aside the objections to the proposed approach that will necessarily be fostered of lack of experience with it, let's now some potential statistical-theory related criticism.

First, it's not unreasonable to see the standard error as the center of statistical inference, at least for the methods likely to be encountered in intro stats. With this view, isn't by-passing the standard error a disadvantage of the proposed approach? Yes, in the sense that it's generally better to know more than less. But teaching the standard error comes with its own costs. Students can be confused by the similar sounding terms "standard deviation" and "standard error." For most purposes, the standard error is only an intermediate result for calculating a confidence interval or a t statistic, and adds a bit of complexity to the overall process.

Second, using F rather than t takes the rug out from under the traditional topic of one-tailed versus two-tailed tests. But one-tailed tests are controversial and easily mis-used. Recently, the American Statistical Association has stated an interest in de-emphasizing p-values as an instrument of inference. [CITE TAS] A lead editorial in this journal (TAS) stated:

> *We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term "statistically significant" entirely. Nor should variants such as "significantly different," "p < 0.05," and "nonsignificant" survive, whether expressed in words, by asterisks in a table, or in some other way.*

In this context, it hardly seems worthwhile to encourage students to distinguish between 0.10 and 0.05 as particular values of an arbitrary threshold. Besides, there are much more important factors at work in constructing a meaningful p-value that are not covered quantitatively in introductory statistics, e.g. the problems of "researcher degrees of freedom," multiple testing, and covariates.

Third, no room has been given in the approach proposed here for calculating confidence intervals and p-values in the so-called "one-sample" setting. But these settings can be taught in other non-traditional ways, for instance by bootstrapping. And, insofar as one-sample settings are introduced to lead students to "two-sample" and other inference procedures involving explanatory variables, this is not a central loss.

Fourth, the graphical approach for calculating p-values (Figure 6) and bounds of confidence intervals (Figure 5) does not result in sufficient precision. The shorthand of using 2 for what should be z* or t* dramatically understates the width of the confidence interval and overstates the p-value for n ≾ 10. In response, I offer two suggestions. 1. Arguably the dominant interest in data in today's world is in large n. 2. The presentation in Figures 5 and 6 actually does a good job in showing the situation for small n. Figure 5 can easily be augmented to show individual curves for whatever values of n are desired. Figure 6 already does this for the 0.05 critical values.

Fifth, the chi-squared test is not incorporated in the proposed approach.  [1. There's no standard error in chi-squared. 2. You get to work with discrete response variables in more generality with a modeling approach. 3. There's nothing to prevent an instructor from continuing to teach chi-squared in the traditional way alongside the proposed approach.]
\


# Conclusion

Whether it makes sense to build inference from standard errors (the traditional approach) or from R and F (the approach proposed here), depends on the instructor's priorities. The standard-error approach was developed in the context of small $n$ and costly computation. (Sufficient statistics support a computationally light, two-phase approach to inferential calculation.) But it does not generalize to the use of multiple explanatory variables or even to multiple levels of a single categorical explanatory variable. 

The R/F procedure unifies and generalizes well to inferential settings with covariates. R/F imposes a heavier computational load (to calculate model values), although a graphical approach to pedagogy allows approximate results to be constructed with calculations by eye.

Both the traditional and the R/F approaches can be streamlined considerably by putting aside situations with small $n$ and the consequent need for tables of critical values. The R/F approach obviates any consideration of dubious aspects of traditional inference such as one-tailed p-values or the spurious precision of the "unequal variance" t-test. R/F, being so closely related to ANOVA, also provides a meaningful basis for introducing the comparison of multiple models, something absent from the traditional approach and it's emphasis on the "single appropriate model" which is, in applications, not so often actually appropriate.



# FROM BEFORE

The orthodox sequence of inferential topics in a college-level introductory statistics course is familiar to instructors. At the core of the sequence are two descriptive, "one-sample" statistics: the sample mean (for quantitative variables) and the sample proportion (for dicotomous categorical variables). The sampling distributions of these statistics are considered, with the standard errors generally being expressed as formulas, i.e. $s/\sqrt{n}$ and $\sqrt{p (1-p) / n}$. These standard errors are used to construct confidence intervals and to perform hypothesis tests. 

The sequence moves on to "two-sample" statistics which compare the sample means or the sample proportions in two groups. Again, procedures are based on standard errors, which in this context are rather more complicated than in the one-sample settings. Depending on the available time, the sequence may continue on to cover the slope of a simple regression line, the chi-squared test, and sometimes "one-way" ANOVA. 

Among the difficulties faced by students are the complexity of calculating with the formulas (particularly in two-sample inference), the proliferation of similar sounding vocabulary (e.g. standard deviation, standard error, margin of error), confusion about which test to apply when, and students regarding the formulas as arbitrary rather than representing the underlying logical model of sampling. 

These difficulties have been addressed by instructors and textbook authors in several ways. Over the last decade, mainstream textbooks have been introduced that use simulation, resampling, and randomization to make more intuitive the link between sampling variation,  confidence intervals, and hypothesis tests. [Lock, Tintle, Open Intro] Computers can be used to streamline calculations and have a broader benefit of supporting data storage and management, graphical presentation, modeling, and simulation.  Calculators, still widely in use in mathematics classrooms despite having been replaced by computers in all other settings, provide a narrow solution to the finding numerical values of the inference formulas.

Important criticisms of the orthodox inference sequence include it failing to address issues of confounding, covariates, and adjustment; its too-intense focus on statistical "significance" (as opposed to genuine significance in a practical realm); and its lack of connection to contemporary methods of modeling with data. [Cite Horton] 

A longer standing criticism stems from a perceived over-emphasis on theoretical probability rather than data. In the 1960s, John Tukey introduced "data analysis" to refocus statistical thinking on data. [Cite Tukey future of data analysis] Over the decades since, this has evolved into "data science." [Cite 50 years of data science]  The recent push by the American Statistical Association to "move beyond p < 0.05" [Cite recent TAS] and to abandon the use of "significant" in statistical nomenclature poses additional challenges to orthodox inference. 

In this paper, I propose a pedagogical approach to inference that emphasizes the connections between the sample mean, sample proportion, and regression contexts, that provides a smooth path from "two-sample" and simple regression situations to multiple regression, that can be used in a classroom even without computers, and that always leaves the original data in plain view. Section 2 introduces the approach, Section 3 considers limitations and possible objections, and Section 4 discusses some salutary implications for introductory statistics in the emerging world of data science.


# Taken out

The graphics described in this section are point plots of data were annotated with models of the response variable as a function of the explanatory variable. When the criterion for fitting a model is least sum of squares, the models are the familiar ones in introductory statistics: means, proportions, and least-square regression lines. 

Of course, the same format of graphic can be made with models constructed using other criteria. Of particular interest pedagogically is the criterion of a student's judgement of what constitutes a pretty good fit. Using this criterion enables trained students to work entirely with a graphical representation of data, without needing explicit computation. Each of the situations (a) through (d) is amenable to approximation by eye of the groupwise means or the straight-line model. Similarly, the standard deviations used in the rug plots can be estimated by eye. 



ANOVA

MULTIPLE REGRESSION

ROBUST REGRESSION

HAND CALCULATIONS


```{r eval = FALSE, echo = FALSE}
Fframe = list()
for (n in c(3, 5, 10, 20, 50, 100, 200, 10000)) {
  Fframe[[n]] <- 
    data.frame(F = seq(.5, 20, length = 100)) %>%
    mutate( p = 1 - pf(F,  1, n-1),
            n = n)
}
Fframe <-  dplyr::bind_rows(Fframe) %>%
  mutate(n = factor(n))
gf_line(p ~ F, data = Fframe, color  = ~n) +
  scale_y_log10(breaks = c(0.1, 0.05, .002, 0.01, .005,  0.001),
                minor_breaks = NULL, limits = c(.0005, .2) ) +
  scale_x_continuous(breaks  = 1:20)
```



